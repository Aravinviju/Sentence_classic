{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stop_words\n",
      "  Downloading stop-words-2015.2.23.1.tar.gz\n",
      "Building wheels for collected packages: stop-words\n",
      "  Running setup.py bdist_wheel for stop-words ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dl1/.cache/pip/wheels/22/74/80/77275c2f9f2f1d9841b51e169a38985640a10fbd2711d10791\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2015.2.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.3.0-cp36-cp36m-manylinux1_x86_64.whl (22.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 22.5MB 82kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.5.6.tar.gz\n",
      "Requirement already satisfied: six>=1.5.0 in /home/dl1/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/dl1/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/dl1/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: boto>=2.32 in /home/dl1/anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Collecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in /home/dl1/anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading boto3-1.5.35-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/dl1/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/dl1/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/dl1/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dl1/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 2.6MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting botocore<1.9.0,>=1.8.49 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading botocore-1.8.49-py2.py3-none-any.whl (4.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.1MB 338kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: docutils>=0.10 in /home/dl1/anaconda3/lib/python3.6/site-packages (from botocore<1.9.0,>=1.8.49->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/dl1/anaconda3/lib/python3.6/site-packages (from botocore<1.9.0,>=1.8.49->boto3->smart-open>=1.2.1->gensim)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dl1/.cache/pip/wheels/36/48/35/97efc2bd1b233627131c9a936c9de23681846db707b907d353\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dl1/.cache/pip/wheels/31/9c/20/996d65ca104cbca940b1b053299b68459391c01c774d073126\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.5.35 botocore-1.8.49 bz2file-0.98 gensim-3.3.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.5.6\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "# doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "# doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "# doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "# doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "# doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "doc_a = 'A computer is a great invention of the modern technology. It is generally a machine which has capability to store large data value in its memory. It works using input (like keyboard) and output (like printer) devices. It is very simple to handle the computer as its functioning is so common that a child can handle it. It is a very reliable device which we can carry with us and use anywhere and anytime. It allows us to make changes in the already stored data as well as store new data. Computer is a new technology which is used in offices, banks, educational institutions, etc.'\n",
    "\n",
    "\n",
    "# compile sample documents into a list\n",
    "# doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "doc_set = [doc_a]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    #stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.023*\"data\" + 0.023*\"comput\"'), (1, '0.044*\"use\" + 0.044*\"store\"')]\n"
     ]
    }
   ],
   "source": [
    " print(ldamodel.print_topics(num_topics=2, num_words=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
