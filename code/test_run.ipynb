{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "if (sys.version_info > (3, 0)):\n",
    "    import pickle as pkl\n",
    "else: #Python 2.7 imports\n",
    "    import cPickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: algorithmia in /home/dl1/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /home/dl1/anaconda3/lib/python3.6/site-packages (from algorithmia)\n",
      "Requirement already satisfied: requests in /home/dl1/anaconda3/lib/python3.6/site-packages (from algorithmia)\n",
      "Requirement already satisfied: enum34 in /home/dl1/anaconda3/lib/python3.6/site-packages (from algorithmia)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/dl1/anaconda3/lib/python3.6/site-packages (from requests->algorithmia)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/dl1/anaconda3/lib/python3.6/site-packages (from requests->algorithmia)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/dl1/anaconda3/lib/python3.6/site-packages (from requests->algorithmia)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dl1/anaconda3/lib/python3.6/site-packages (from requests->algorithmia)\n"
     ]
    }
   ],
   "source": [
    "!pip install algorithmia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We download English word embeddings from here https://www.cs.york.ac.uk/nlp/extvec/\n",
    "embeddingsPath = '/home/dl1/Arav/Neuralnets/Session 2 - Sentence CNN/code/embeddings/wiki_extvec.gz'\n",
    "\n",
    "#Train, Dev, and Test files\n",
    "folder = '/home/dl1/Arav/Neuralnets/Session 2 - Sentence CNN/code/data/'\n",
    "files = [folder+'train.txt',  folder+'dev.txt', folder+'test.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createMatrices(sentences, word2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "    \n",
    "    \n",
    "    xMatrix = []\n",
    "    unknownWordCount = 0\n",
    "    wordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        targetWordIdx = 0\n",
    "        \n",
    "        sentenceWordIdx = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            wordCount += 1\n",
    "            \n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]\n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "                \n",
    "            sentenceWordIdx.append(wordIdx)\n",
    "            \n",
    "        xMatrix.append(sentenceWordIdx)\n",
    "       \n",
    "    \n",
    "    print(\"Unknown tokens: %.2f%%\" % (unknownWordCount/(float(wordCount))*100))\n",
    "    return xMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "    sentences = []    \n",
    "    labels = []\n",
    "    \n",
    "    for line in open(filepath):   \n",
    "        splits = line.split()\n",
    "        label = int(splits[0])\n",
    "        words = splits[1:]\n",
    "        \n",
    "        labels.append(label)\n",
    "        sentences.append(words)\n",
    "        \n",
    "    print(filepath, len(sentences), \"sentences\")\n",
    "    \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dl1/Arav/Neuralnets/Session 2 - Sentence CNN/code/data/train.txt 5330 sentences\n",
      "/home/dl1/Arav/Neuralnets/Session 2 - Sentence CNN/code/data/dev.txt 2664 sentences\n",
      "/home/dl1/Arav/Neuralnets/Session 2 - Sentence CNN/code/data/test.txt 2668 sentences\n"
     ]
    }
   ],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: #\n",
    "#      Start of the preprocessing\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: #\n",
    "\n",
    "outputFilePath = '/home/dl1/Arav/Neuralnets/Session 2 - Sentence CNN/code/pkl/data`.pkl.gz'\n",
    "\n",
    "\n",
    "trainDataset = readFile(files[0])\n",
    "devDataset = readFile(files[1])\n",
    "testDataset = readFile(files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# :: Compute which words are needed for the train/dev/test set ::\n",
    "words = {}\n",
    "for sentences, labels in [trainDataset, devDataset, testDataset]:       \n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            words[token.lower()] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# :: Read in word embeddings ::\n",
    "word2Idx = {}\n",
    "wordEmbeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # :: Downloads the embeddings from the York webserver ::\n",
    "# if not os.path.isfile(embeddingsPath):\n",
    "#     basename = os.path.basename(embeddingsPath)\n",
    "#     if basename == 'wiki_extvec.gz':\n",
    "# \t       print(\"Start downloading word embeddings for English using wget ...\")\n",
    "# \t       #os.system(\"wget https://www.cs.york.ac.uk/nlp/extvec/\"+basename+\" -P embeddings/\") #Original path from York University\n",
    "# \t       os.system(\"wget https://public.ukp.informatik.tu-darmstadt.de/reimers/2017_english_embeddings/\"+basename+\" -P embeddings/\")\n",
    "#     else:\n",
    "#         print(embeddingsPath, \"does not exist. Please provide pre-trained embeddings\")\n",
    "#         exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# :: Load the pre-trained embeddings file ::\n",
    "fEmbeddings = gzip.open(embeddingsPath, \"r\") if embeddingsPath.endswith('.gz') else open(embeddingsPath, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fEmbeddings\n",
    "# word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "# word2Idx\n",
    "\n",
    "# vector = np.zeros(3)\n",
    "# vector\n",
    "# wordEmbeddings.append(vector)\n",
    "# wordEmbeddings\n",
    "\n",
    "# for line in fEmbeddings:\n",
    "    \n",
    "#     split = line.decode(\"utf-8\").strip().split(\" \")\n",
    "# #     break\n",
    "# print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-trained embeddings file\n",
      "Embeddings shape:  (16554, 300)\n",
      "Len words:  21347\n"
     ]
    }
   ],
   "source": [
    "print(\"Load pre-trained embeddings file\")\n",
    "for line in fEmbeddings:\n",
    "    split = line.decode(\"utf-8\").strip().split(\" \")\n",
    "    word = split[0]\n",
    "    \n",
    "    if len(word2Idx) == 0: #Add padding+unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector for 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if word.lower() in words:\n",
    "        vector = np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word2Idx[word] = len(word2Idx)\n",
    "       \n",
    "        \n",
    "wordEmbeddings = np.array(wordEmbeddings)\n",
    "\n",
    "print(\"Embeddings shape: \", wordEmbeddings.shape)\n",
    "print(\"Len words: \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wordEmbeddings\n",
    "# word2Idx\n",
    "# trainDataset[1]\n",
    "# word2Idx['UNKNOWN_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown tokens: 4.86%\n",
      "Unknown tokens: 4.86%\n",
      "Unknown tokens: 4.77%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# :: Create matrices ::\n",
    "train_matrix = createMatrices(trainDataset[0], word2Idx)\n",
    "dev_matrix = createMatrices(devDataset[0], word2Idx)\n",
    "test_matrix = createMatrices(testDataset[0], word2Idx)\n",
    "\n",
    "\n",
    "data = {\n",
    "    'wordEmbeddings': wordEmbeddings, 'word2Idx': word2Idx,\n",
    "    'train': {'sentences': train_matrix, 'labels': trainDataset[1]},\n",
    "    'dev':   {'sentences': dev_matrix, 'labels': devDataset[1]},\n",
    "    'test':  {'sentences': test_matrix, 'labels': testDataset[1]}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(test_matrix)\n",
    "# testDataset[1]\n",
    "# trainDataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"The greatest pleasure in life is doing what people say you cannot do.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splits = text.split()\n",
    "testwords = splits\n",
    "# testwords\n",
    "# sentences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createtestMatrix(sentence, word2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "    \n",
    "    \n",
    "    testMatrix = []\n",
    "    unknownWordCount = 0\n",
    "    wordCount = 0\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "    targetWordIdx = 0\n",
    "\n",
    "#     sentenceWordIdx = []\n",
    "\n",
    "    for word in sentence:\n",
    "        wordCount += 1\n",
    "\n",
    "        if word in word2Idx:\n",
    "            wordIdx = word2Idx[word]\n",
    "        elif word.lower() in word2Idx:\n",
    "            wordIdx = word2Idx[word.lower()]\n",
    "        else:\n",
    "            wordIdx = unknownIdx\n",
    "            unknownWordCount += 1\n",
    "\n",
    "#         sentenceWordIdx.append(wordIdx)\n",
    "\n",
    "        testMatrix.append(wordIdx)\n",
    "\n",
    "    \n",
    "    print(\"Unknown tokens in test_Text: %.2f%%\" % (unknownWordCount/(float(wordCount))*100))\n",
    "\n",
    "    return testMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown tokens in test_Text: 18.10%\n"
     ]
    }
   ],
   "source": [
    "finalTest_matrix = createtestMatrix(testwords, word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalTest_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultFilePath = '/home/dl1/Arav/Neuralnets/Session 2 - Sentence CNN/code/pkl/resultdata`.pkl.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testarray = np.array(finalTest_matrix)\n",
    "testarray = testarray.reshape(1,len(finalTest_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 105)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testarray.tolist()\n",
    "testarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = gzip.open(resultFilePath, 'wb')\n",
    "pkl.dump(testarray, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textLDA=[]\n",
    "textLDA.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'banks': 1, 'capability': 1, 'device': 1, 'educational': 1, 'functioning': 1, 'institutions': 1, 'large': 1, 'works': 1}, {'carry': 1, 'devices': 1, 'handle': 2, 'make': 1, 'printer': 1, 'reliable': 1, 'stored': 1, 'technology': 2}, {'child': 1, 'great': 1, 'invention': 1, 'memory': 1, 'offices': 1}, {'anytime': 1, 'common': 1, 'computer': 2, 'data': 3, 'input': 1, 'keyboard': 1, 'simple': 1, 'store': 2}]\n"
     ]
    }
   ],
   "source": [
    "#usage of the LDA - Algorithmia for \n",
    "\n",
    "import Algorithmia\n",
    "\n",
    "input = {\n",
    "  \"docsList\": textLDA,\n",
    "  \"mode\": \"quality\"\n",
    "}\n",
    "client = Algorithmia.client('sim+KZtb16R1rtOXC0dk9Y4sqEb1')\n",
    "algo = client.algo('nlp/LDA/1.0.0')\n",
    "LDAresult = algo.pipe(input).result\n",
    "print(LDAresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banks': 1, 'capability': 1, 'device': 1, 'educational': 1, 'functioning': 1, 'institutions': 1, 'large': 1, 'works': 1}\n",
      "{'carry': 1, 'devices': 1, 'handle': 2, 'make': 1, 'printer': 1, 'reliable': 1, 'stored': 1, 'technology': 2}\n",
      "{'child': 1, 'great': 1, 'invention': 1, 'memory': 1, 'offices': 1}\n",
      "{'anytime': 1, 'common': 1, 'computer': 2, 'data': 3, 'input': 1, 'keyboard': 1, 'simple': 1, 'store': 2}\n"
     ]
    }
   ],
   "source": [
    "# jsontopython = json.load(LDAresult)\n",
    "# print(jsontopython)\n",
    "for item in LDAresult:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
